{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaveshsingh0206/Contextual-Embedding-based-Stock-Price-Prediction-using-Bidirectional-LSTM/blob/main/Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfX-Tcu7qaFv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install gensim\n",
        "!pip install sns\n",
        "!pip install torchvision "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V71BfOKhJub7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "006NPFERj6kE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrN9DBH51Qb4"
      },
      "outputs": [],
      "source": [
        "text=[]\n",
        "sentiment=[]\n",
        "with open('/content/drive/MyDrive/NLP Dataset/FinancialPhraseBank-v1.0 2/Sentences_50Agree.txt', encoding = \"ISO-8859-1\") as f:\n",
        "  lines = f.read().splitlines()\n",
        "  for line in lines:\n",
        "    line=line.rstrip().split('@')\n",
        "    text.append(' '.join(line[:-1]))\n",
        "    sentiment.append(line[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dJXpVT64gzT"
      },
      "outputs": [],
      "source": [
        "data = {'Text': text, 'Sentiment': sentiment}  \n",
        "df = pd.DataFrame(data)  \n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao8kXyBVFeiO"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku9-JCvfF1d9"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da89zM7VGCI9"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOma2JV-_JmF"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp9eevAt_DGM"
      },
      "outputs": [],
      "source": [
        "punctuations = '''!()-[]{};:\"\\,<>./?@#%^&*_~'''\n",
        "def cleanText(x):\n",
        "  x = x.lower().strip()\n",
        "  soup = BeautifulSoup(x)\n",
        "  x = soup.get_text()\n",
        "  x = re.sub(r'https?://\\S+', '', x)\n",
        "  x=re.sub(\"\\s\\s+\", \" \", x.strip())\n",
        "  no_punct = \"\"\n",
        "  for char in x:\n",
        "    if char not in punctuations:\n",
        "        no_punct = no_punct + char\n",
        "    else:\n",
        "        no_punct += \" \"\n",
        "  x = no_punct\n",
        "\n",
        "  x=re.sub(\"\\s\\s+\", \" \", x.strip())\n",
        "  if (x==' ' or len(x)==0): \n",
        "    return np.nan\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1GkuwkV_FdP"
      },
      "outputs": [],
      "source": [
        "df['Text']= df['Text'].apply(cleanText)\n",
        "maxLen = 0\n",
        "for l in df['Text']:\n",
        "  maxLen = max(maxLen, len(l.split(' ')))\n",
        "print(maxLen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw6rswl1GRfZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "labels = {'neutral':0,\n",
        "          'negative':1,\n",
        "          'positive':2,\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coRH6rYXM0sX"
      },
      "outputs": [],
      "source": [
        "def change_label(x):\n",
        "  return labels[x]\n",
        "df['Sentiment_Label']=df['Sentiment'].apply(change_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4nT0KG1T4SG"
      },
      "outputs": [],
      "source": [
        "# /df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoyqsSi9Gg27"
      },
      "outputs": [],
      "source": [
        "labels = {'neutral':0,\n",
        "          'negative':1,\n",
        "          'positive':2,\n",
        "          }\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.labels = [labels[label] for label in df['Sentiment']]\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                            return_tensors=\"pt\") for text in df['Text']]\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jNoUJPBHVAR"
      },
      "outputs": [],
      "source": [
        "np.random.seed(112)\n",
        "df_train, df_val = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.9*len(df))])\n",
        "\n",
        "print(len(df_train),len(df_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF-cy398IeGf"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVKpAqvZHnjq"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        # print(input_id.shape)\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        # print(dropout_output.shape)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        # print(linear_output.shape)\n",
        "        final_layer = self.relu(linear_output)\n",
        "        # print(final_layer.shape)\n",
        "        return final_layer\n",
        "    \n",
        "    def get_vector(self, input_id, mask):\n",
        "      _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
        "      dropout_output = self.dropout(pooled_output)\n",
        "      return dropout_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMpf5f6eJVTI"
      },
      "outputs": [],
      "source": [
        "# batch, max_seq_len, dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYy_OThRHwzs"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                batch_loss = criterion(output, train_label.long())\n",
        "                # break\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            # break\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label.long())\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "                  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "bert_pretrained_model = BertClassifier()                        \n",
        "LR = 1e-6                                                                                                                       \n",
        "train(bert_pretrained_model, df_train, df_val, LR, EPOCHS)"
      ],
      "metadata": {
        "id": "oSxS2j1Wu6F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Bert Model\n",
        "# torch.save(bert_pretrained_model.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/bert_pretrained_model.pt')\n",
        "bert_pretrained_model = BertClassifier()\n",
        "bert_pretrained_model.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/bert_pretrained_model.pt'))"
      ],
      "metadata": {
        "id": "4KpN9YyLuMNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMxuvKv3vAjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OPxFjfFtKnUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqpunhN-q--"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=True, dropout=0.2)   \n",
        "        self.linear = nn.Linear(self.hidden_dim*4 , 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.out = nn.Linear(64, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        h_lstm, _ = self.gru(x, hidden)\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
        "        conc = self.relu(self.linear(conc))\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.out(conc)\n",
        "        return out\n",
        "    \n",
        "    def get_vector(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "        h_lstm, _ = self.gru(x, hidden)\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
        "        conc = self.linear(conc)\n",
        "        conc = self.dropout(conc)\n",
        "        return conc\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
        "        return hidden.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=True, dropout=0.2)   \n",
        "        self.linear = nn.Linear(self.hidden_dim*4 , 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.out = nn.Linear(64, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        hidden, c = self.init_hidden(batch_size)\n",
        "        h_lstm, _ = self.lstm(x, (hidden, c))\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
        "        conc = self.relu(self.linear(conc))\n",
        "        conc = self.dropout(conc)\n",
        "        out = self.out(conc)\n",
        "        return out\n",
        "    \n",
        "    def get_vector(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        hidden, c = self.init_hidden(batch_size)\n",
        "        h_lstm, _ = self.lstm(x, (hidden, c))\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
        "        conc = self.linear(conc)\n",
        "        conc = self.dropout(conc)\n",
        "        return conc\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)\n",
        "        return hidden.to(device), c.to(device)"
      ],
      "metadata": {
        "id": "7VWWai2MKSX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkpy4QKcKOcm"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "word2Vec = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfSvMTzIW_cB"
      },
      "outputs": [],
      "source": [
        "'Sunday' in word2Vec.wv.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRc5c-mTLFZQ"
      },
      "outputs": [],
      "source": [
        "XSentences = []\n",
        "import math\n",
        "for i, sent in enumerate(df['Text'].tolist()):\n",
        "    sent = sent.split(' ')\n",
        "    temp = []\n",
        "    for word in sent:\n",
        "        if word in word2Vec.wv.vocab:\n",
        "            temp.append(word2Vec[word])\n",
        "    if(len(temp)==0):\n",
        "        temp.append(np.zeros(300))\n",
        "    XSentences.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0OIhtHZXs2h"
      },
      "outputs": [],
      "source": [
        "print(len(XSentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILiv1vGaK518"
      },
      "outputs": [],
      "source": [
        "def createXForRNN(l, XSentences):\n",
        "    X = np.zeros([len(XSentences), l, 300])\n",
        "    for i, sent in enumerate(XSentences):\n",
        "        for j in range(0, l):\n",
        "            if(j<len(sent)):\n",
        "                word = sent[j]\n",
        "                temp=word\n",
        "            else:\n",
        "                temp = np.zeros(300)\n",
        "\n",
        "            X[i][j] = temp\n",
        "    return X\n",
        "X_rnn = createXForRNN(maxLen, XSentences)\n",
        "print(X_rnn.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhtdSl83NK-L"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 2\n",
        "def createDataLoader(X_train, Y_train, X_test, Y_test):\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "  Y_train = torch.tensor(Y_train,  dtype=torch.long).to(device)\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "  Y_test = torch.tensor(Y_test,  dtype=torch.long).to(device)\n",
        "  train = torch.utils.data.TensorDataset(X_train, Y_train)\n",
        "  test = torch.utils.data.TensorDataset(X_test, Y_test)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWOOQK5CMIY-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_rnn, np.array(df['Sentiment_Label'].tolist()), test_size=0.1, random_state=42)\n",
        "train_loader, test_loader = createDataLoader(X_train, Y_train, X_test, Y_test)\n",
        "import time\n",
        "def evaluate(n_epochs, model, optimizer, train_loader, test_loader, criterion):\n",
        "    def calculateCorrect(t, y):\n",
        "        sum=0\n",
        "        for i, v in enumerate(t):\n",
        "            if(t[i]==y[i]):\n",
        "                sum+=1\n",
        "        return sum\n",
        "\n",
        "    for e in tqdm(range(n_epochs)):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        avg_loss = 0.  \n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item() / len(train_loader)\n",
        "      \n",
        "        model.eval()        \n",
        "        avg_val_loss = 0.\n",
        "        val_preds = np.zeros((len(X_test), 3))\n",
        "      \n",
        "        for i, (x_batch, y_batch) in enumerate(test_loader):\n",
        "            y_pred = model(x_batch).detach()\n",
        "            avg_val_loss += criterion(y_pred, y_batch).item() / len(test_loader)\n",
        "            val_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = F.softmax(y_pred).cpu().numpy()\n",
        "          \n",
        "        accuracy = calculateCorrect(val_preds.argmax(axis=1), Y_test)/len(Y_test)\n",
        "        elapsed_time = time.time() - start_time \n",
        "        print('Epoch {}/{} \\t loss={:.4f} \\t test_loss={:.4f}  \\t test_acc={:.4f}  \\t time={:.2f}s'.format(\n",
        "                  e + 1, n_epochs, avg_loss, avg_val_loss, accuracy, elapsed_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2FHqM59JjDA"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model1 = LSTMModel(300, 3, 20, 2)\n",
        "optimizer_rnn = Adam(model1.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model1.to(device)\n",
        "\n",
        "evaluate(20, model1, optimizer_rnn, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LSTM Model\n",
        "# torch.save(model1.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/lstm.pt')\n",
        "model1 = LSTMModel(300, 3, 20, 2)\n",
        "model1.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/lstm.pt'))\n",
        "# model1 = torch.load('/content/drive/MyDrive/NLP Dataset/model/lstm.pt')"
      ],
      "metadata": {
        "id": "cNgRjH2z-KR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2 = GRUModel(300, 3, 20, 2)\n",
        "optimizer2 = Adam(model2.parameters(), lr=0.001)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "model2.to(device)\n",
        "\n",
        "evaluate(20, model2, optimizer2, train_loader, test_loader, criterion2)"
      ],
      "metadata": {
        "id": "C3mE_ao4Kxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yVGZWoMNmCq"
      },
      "outputs": [],
      "source": [
        "# Save GRU Model\n",
        "torch.save(model2.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/gru.pt')\n",
        "# model2 = GRUModel(300, 3, 20, 2)\n",
        "# model2.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/gru.pt'))\n",
        "# model2 = torch.load('/content/drive/MyDrive/NLP Dataset/model/gru.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN3CeLwFNmZn"
      },
      "source": [
        "Time Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPCEs4wI6g48"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "ts = pd.read_csv('/content/drive/MyDrive/NLP Dataset/choti_bhabhi.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbmYhzuY62aX"
      },
      "outputs": [],
      "source": [
        "ts.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts.isna().sum()"
      ],
      "metadata": {
        "id": "ALyi0DpunuJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts.info()"
      ],
      "metadata": {
        "id": "abYFpjwIoGZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ts['yahoo'][0].split(' ')))"
      ],
      "metadata": {
        "id": "gQe-jTxGlgH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "def createXForTimeSeries(ts, lenX):\n",
        "  ts['split'] = ts['yahoo'].str.slice(1, -1)\n",
        "  ts=ts.join(ts['split'].str.split(',', -1, expand=True).astype(float))\n",
        "  temp = ts[[x for x in range(0, lenX+1)]]\n",
        "  temp[[x for x in range(0, lenX+1)]] = scaler.fit_transform(temp[[x for x in range(0, lenX+1)]])\n",
        "  X = temp[[x for x in range(0, lenX)]]\n",
        "  X = np.array(X)\n",
        "  X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "  Y = ts[[lenX]]\n",
        "  Y = np.array(Y)\n",
        "  Y = Y.reshape(Y.shape[0], )\n",
        "  X_Next = ts[[x for x in range(lenX)]]\n",
        "  X_Next = np.array(X_Next)\n",
        "  X_Next = X_Next.reshape(X_Next.shape[0], X_Next.shape[1], 1)\n",
        "  return ts, X, Y, X_Next"
      ],
      "metadata": {
        "id": "SrSmIZ2UmXNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y[0]"
      ],
      "metadata": {
        "id": "JYcF0-WaxC1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ts, X, Y, X_Next = createXForTimeSeries(ts, 100)\n",
        "print(X.shape, Y.shape)"
      ],
      "metadata": {
        "id": "77-u_ipsqKzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Next.shape "
      ],
      "metadata": {
        "id": "dVlkXhhDCIkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts.head()"
      ],
      "metadata": {
        "id": "-LnvM2dBnaSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts['Sentiment'] = 'neutral'\n",
        "ts['Text'] = ts['clean_title']"
      ],
      "metadata": {
        "id": "wzfORYZuUnjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts.head()"
      ],
      "metadata": {
        "id": "qll1U-hK1F4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lC86NZj64jb"
      },
      "outputs": [],
      "source": [
        "class TimeSeries(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "    super(TimeSeries, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "    self.gru = nn.GRU(input_size, hidden_dim, n_layers, batch_first=True, bidirectional=False, dropout=0.2)   \n",
        "    self.linear = nn.Linear(self.hidden_dim*self.n_layers , 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    self.out1 = nn.Linear(64, 32)\n",
        "    self.out2 = nn.Linear(32, 1)\n",
        "  def forward(self, x, news):\n",
        "    # x-> batch, 100, 1\n",
        "    # news has to be mapped with hidden state\n",
        "    # bert mappes it to batch, 768 -> final bert outout\n",
        "    # num_layers*biredirection, batch, hidden\n",
        "    news = news.unsqueeze(0)\n",
        "    x, _ = self.gru(x, news)\n",
        "    x = x[:, -1, :]\n",
        "    x = self.relu(self.linear(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.out2(self.relu(self.out1(x)))\n",
        "    # batch, 1\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateTimeSeries(n_epochs, model, optimizer, train_loader, test_loader, criterionT):\n",
        "    for e in tqdm(range(n_epochs)):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        avg_loss = 0.  \n",
        "        for i, (x, news, y, next) in enumerate(train_loader):\n",
        "            y_pred = model(x, news)\n",
        "            y=y.unsqueeze(1)\n",
        "            loss = criterionT(y_pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item() / len(train_loader)\n",
        "      \n",
        "        model.eval()        \n",
        "        avg_val_loss = 0.\n",
        "        # val_preds = np.zeros((len(X_test), 1))\n",
        "      \n",
        "        for i, (x_batch, news_batch, y_batch, next) in enumerate(test_loader):\n",
        "            y_pred = model(x_batch, news_batch).detach()\n",
        "            y_batch=y_batch.unsqueeze(1)\n",
        "            avg_val_loss += criterionT(y_pred, y_batch).item() / len(test_loader)\n",
        "            # val_preds[i * BATCH_SIZE:(i+1) * BATCH_SIZE] = F.softmax(y_pred).cpu().numpy()\n",
        "        elapsed_time = time.time() - start_time \n",
        "        print('Epoch {}/{} \\t test loss={:.4f} \\t time={:.2f}s'.format(\n",
        "                  e + 1, n_epochs, avg_loss, elapsed_time))"
      ],
      "metadata": {
        "id": "Ab1VB5v_fPCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createXNews(ts, bertmodel):\n",
        "  X_news = np.zeros([ts.shape[0], 768])\n",
        "  count=0\n",
        "  for news in tqdm(ts['Text'].tolist()):\n",
        "    data = {'Text': [news], 'Sentiment': ['neutral']}  \n",
        "    newdf = pd.DataFrame(data)  \n",
        "    n = Dataset(newdf)\n",
        "    train_dataloader = torch.utils.data.DataLoader(n, batch_size=1)\n",
        "    for train_input, _ in train_dataloader:\n",
        "      mask = train_input['attention_mask'].to(device)\n",
        "      input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "      output = bertmodel.get_vector(input_id, mask)\n",
        "      X_news[count] = output[0].cpu().detach().numpy()\n",
        "      count+=1\n",
        "  return X_news"
      ],
      "metadata": {
        "id": "Q2AR-tl9TykO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XNewsSentences = []\n",
        "import math\n",
        "maxSequenceLen = 0\n",
        "for i, sent in enumerate(ts['Text'].tolist()):\n",
        "    sent = sent.split(' ')\n",
        "    maxSequenceLen=max(maxSequenceLen, len(sent))\n",
        "    temp = []\n",
        "    for word in sent:\n",
        "        if word in word2Vec.wv.vocab:\n",
        "            temp.append(word2Vec[word])\n",
        "    if(len(temp)==0):\n",
        "        temp.append(np.zeros(300))\n",
        "    XNewsSentences.append(temp)\n",
        "print(maxSequenceLen)\n",
        "def createXNewsForLSTM(l, XNewsSentences):\n",
        "    X = np.zeros([len(XNewsSentences), l, 300])\n",
        "    for i, sent in enumerate(XNewsSentences):\n",
        "        for j in range(0, l):\n",
        "            if(j<len(sent)):\n",
        "                word = sent[j]\n",
        "                temp=word\n",
        "            else:\n",
        "                temp = np.zeros(300)\n",
        "\n",
        "            X[i][j] = temp\n",
        "    return X\n",
        "def createXNewsModelReadyForLSTM(dim, model, maxSequenceLen, XNewsSentences):\n",
        "  X_News_LSTM = createXNewsForLSTM(maxSequenceLen, XNewsSentences)\n",
        "  # X_News_LSTM = torch.tensor(X_News_LSTM, dtype=torch.float32).cpu()\n",
        "  # X_News_LSTM -> total, seq, 300\n",
        "\n",
        "\n",
        "  X_News = torch.tensor(np.zeros([len(X_News_LSTM), dim]))\n",
        "  model.eval()\n",
        "  model.to(device) \n",
        "  for i, news in tqdm(enumerate(X_News_LSTM)):\n",
        "    # news->seq, 300\n",
        "    news = torch.tensor(news, dtype=torch.float32)\n",
        "    news = news.unsqueeze(0)\n",
        "    X_News[i] = model.get_vector(news.to(device))\n",
        "  return X_News\n",
        "\n",
        "# print(total, 64)"
      ],
      "metadata": {
        "id": "ihABfWTkHQW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.zeros([100, 100])\n",
        "# for x in range()"
      ],
      "metadata": {
        "id": "kLM_q9w2_jD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T=torch.tensor(X_News_LSTM[0], dtype=torch.float32).unsqueeze(0)"
      ],
      "metadata": {
        "id": "aZJVrDRIKJGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def createXNewsForLSTM(ts, model, dim):\n",
        "#   X_news = np.zeros([ts.shape[0], dim])\n",
        "#   # count=0\n",
        "#   for news in tqdm(ts['Text'].tolist()):\n",
        "#     pass\n",
        "#   # for news in tqdm(ts['Text'].tolist()):\n",
        "#   #   data = {'Text': [news], 'Sentiment': ['neutral']}  \n",
        "#   #   newdf = pd.DataFrame(data)  \n",
        "#   #   n = Dataset(newdf)\n",
        "#   #   train_dataloader = torch.utils.data.DataLoader(n, batch_size=1)\n",
        "#   #   for train_input, _ in train_dataloader:\n",
        "#   #     mask = train_input['attention_mask'].to(device)\n",
        "#   #     input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "#   #     output = model.get_vector(input_id, mask)\n",
        "#   #     X_news[count] = output[0].cpu().detach().numpy()\n",
        "#   #     count+=1\n",
        "#   return X_news"
      ],
      "metadata": {
        "id": "tbwxULukGP6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createDataLoaderForNews(X_train, News_train, Y_train, X_test, News_test, Y_test, BATCH_SIZE, X_Next_train, X_Next_test):\n",
        "  X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "  Y_train = torch.tensor(Y_train,  dtype=torch.float32).to(device)\n",
        "\n",
        "  News_train = torch.tensor(News_train,  dtype=torch.float32).to(device)\n",
        "  X_Next_train = torch.tensor(X_Next_train,  dtype=torch.float32).to(device)\n",
        "\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "  Y_test = torch.tensor(Y_test,  dtype=torch.float32).to(device)\n",
        "  News_test = torch.tensor(News_test,  dtype=torch.float32).to(device)\n",
        "  X_Next_test = torch.tensor(X_Next_test,  dtype=torch.float32).to(device)\n",
        "\n",
        "  train = torch.utils.data.TensorDataset(X_train, News_train, Y_train, X_Next_train)\n",
        "  test = torch.utils.data.TensorDataset(X_test, News_test, Y_test, X_Next_test)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "9oPWOUeaRwaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8r4YkdeH_G3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_news = np.zeros([ts.shape[0], 768])\n",
        "from tqdm import tqdm\n",
        "# bert_pretrained_model.to(device)\n",
        "model1.to(device)\n",
        "X_news = createXNews(ts, bert_pretrained_model)\n",
        "X_news_LSTM = createXNewsModelReadyForLSTM(64, model1, maxSequenceLen, XNewsSentences)\n",
        "X_news_GRU = createXNewsModelReadyForLSTM(64, model2, maxSequenceLen, XNewsSentences)\n",
        "# X_news_LSTM -> total, 64\n",
        "# _, X, Y, X_Next = createXForTimeSeries(ts, 100)"
      ],
      "metadata": {
        "id": "S4AYd2o8jIFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_news.shape"
      ],
      "metadata": {
        "id": "xo5s_8HoDGsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, X_news_train, X_news_test, Y_train, Y_test, X_Next_train, X_Next_test = train_test_split(X, X_news_LSTM, Y, X_Next, test_size=0.2, random_state=42)\n",
        "# X_train, X_test, X_news_train, X_news_test, Y_train, Y_test, X_Next_train, X_Next_test = train_test_split(X, X_news, Y, X_Next, test_size=0.2, random_state=42)\n",
        "X_train, X_test, X_news_train, X_news_test, Y_train, Y_test, X_Next_train, X_Next_test = train_test_split(X, X_news_GRU, Y, X_Next, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kCjrCTWhQgVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelTimeSeries = TimeSeries(1, 1, 768, 1) #input_size, output_size, hidden_dim, n_layers\n",
        "optimizerT = Adam(modelTimeSeries.parameters(), lr=0.001)\n",
        "criterionT = nn.MSELoss()\n",
        "modelTimeSeries.to(device)\n",
        "predicted_list = [random.randint(-20, 20) for _ in range(101)]\n",
        "train_loader, test_loader = createDataLoaderForNews(X_train, X_news_train, Y_train, X_test, X_news_test, Y_test, 64, X_Next_train, X_Next_test)\n",
        "evaluateTimeSeries(200, modelTimeSeries, optimizerT, train_loader, test_loader, criterionT)"
      ],
      "metadata": {
        "id": "UhqtZ6S4e9_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelTimeSeries = TimeSeries(1, 1, 768, 1)\n",
        "# torch.save(modelTimeSeries.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/modelTimeSeries.pt')\n",
        "modelTimeSeries.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/modelTimeSeries.pt'))"
      ],
      "metadata": {
        "id": "nltNprnhqiWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelTimeSeriesLSTM = TimeSeries(1, 1, 64, 1) #input_size, output_size, hidden_dim, n_layers\n",
        "optimizerT = Adam(modelTimeSeriesLSTM.parameters(), lr=0.001)\n",
        "criterionT = nn.MSELoss()\n",
        "modelTimeSeriesLSTM.to(device)\n",
        "train_loader, test_loader = createDataLoaderForNews(X_train, X_news_train, Y_train, X_test, X_news_test, Y_test, 64, X_Next_train, X_Next_test)\n",
        "evaluateTimeSeries(200, modelTimeSeriesLSTM, optimizerT, train_loader, test_loader, criterionT)"
      ],
      "metadata": {
        "id": "4AwqnXwvSB1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelTimeSeriesLSTM = TimeSeries(1, 1, 64, 1)\n",
        "# torch.save(modelTimeSeriesLSTM.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/modelTimeSeriesLSTM.pt')\n",
        "modelTimeSeriesLSTM.to(device)\n",
        "modelTimeSeriesLSTM.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/modelTimeSeriesLSTM.pt'))"
      ],
      "metadata": {
        "id": "-BYOpHONsJpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KrXAvLHzE5Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelTimeSeriesGRU = TimeSeries(1, 1, 64, 1) #input_size, output_size, hidden_dim, n_layers\n",
        "optimizerT = Adam(modelTimeSeriesGRU.parameters(), lr=0.001)\n",
        "criterionT = nn.MSELoss()\n",
        "modelTimeSeriesGRU.to(device)\n",
        "train_loader, test_loader = createDataLoaderForNews(X_train, X_news_train, Y_train, X_test, X_news_test, Y_test, 64, X_Next_train, X_Next_test)\n",
        "evaluateTimeSeries(200, modelTimeSeriesGRU, optimizerT, train_loader, test_loader, criterionT)"
      ],
      "metadata": {
        "id": "vv9OQEYLse7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelTimeSeriesGRU = TimeSeries(1, 1, 64, 1)\n",
        "\n",
        "# torch.save(modelTimeSeriesGRU.state_dict(), '/content/drive/MyDrive/NLP Dataset/model/modelTimeSeriesGRU.pt')\n",
        "modelTimeSeriesGRU.load_state_dict(torch.load('/content/drive/MyDrive/NLP Dataset/model/modelTimeSeriesGRU.pt'))"
      ],
      "metadata": {
        "id": "8iU5oEiOsrCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createChart(model, news, x, y, time):\n",
        "  y_pred = model(x, news).detach()\n",
        "  y = y.cpu().detach().numpy()\n",
        "  \n",
        "  pass"
      ],
      "metadata": {
        "id": "GiwVABjlj8lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KUIj-Z1XfdBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "def calculateRSquare(test_loader, model):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  for i, (x_batch, news_batch, y_batch, x_next) in enumerate(test_loader):\n",
        "    y_predicted = model(x_batch, news_batch).detach()\n",
        "    y_batch = y_batch.cpu().detach().numpy()\n",
        "    y_predicted = y_predicted.cpu().numpy()\n",
        "    for i in range(len(y_predicted)):\n",
        "      y_true.append((y_batch[i]))\n",
        "      y_pred.append((y_predicted[i][0]))\n",
        "  print(r2_score(y_true, y_pred))"
      ],
      "metadata": {
        "id": "HUQAUI3wgJ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculateRSquare(test_loader, modelTimeSeriesGRU)"
      ],
      "metadata": {
        "id": "Mk5VugqNkFSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculateRSquare(test_loader, modelTimeSeriesLSTM)"
      ],
      "metadata": {
        "id": "REnQPlTVQtU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculateRSquare(test_loader, modelTimeSeries)"
      ],
      "metadata": {
        "id": "ouWgYezpOreI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def draw(y_actual, y_pred):\n",
        "    num_rows = 101\n",
        "    y_actual=y_actual.reshape(-1)\n",
        "    y_pred=y_pred.reshape(-1)\n",
        "    data_preproc = pd.DataFrame({'Days':[x for x in range(1, 102)], 'True Values': y_actual, 'Predicted Values': y_pred})\n",
        "        \n",
        "    sns.lineplot(x='Days', y='value', hue='variable', data=pd.melt(data_preproc, ['Days']))\n",
        "def predict_graph(loader, scaler, model):\n",
        "  for i, (x_batch, news_batch, y_batch, x_next) in enumerate(loader):\n",
        "    index=60\n",
        "    x=x_batch[index].unsqueeze(0)\n",
        "    news=news_batch[index].unsqueeze(0)\n",
        "    y_predicted_list = []\n",
        "    y_new_pred = x_next[index].detach().cpu().numpy().reshape(-1)\n",
        "    for k in range(100):\n",
        "      y_pred=model(x, news)\n",
        "      y_new_pre = y_pred.detach().cpu().numpy()\n",
        "      y = y_new_pred[k]+predicted_list[k]\n",
        "      y_predicted_list.append(y)\n",
        "      x=x.detach().cpu().numpy()\n",
        "      x=np.append(x, 1)\n",
        "      x=x.reshape(1, -1)\n",
        "      x=scaler.transform(x)\n",
        "      x = x.reshape(-1, 1)\n",
        "      x = x[:-1]\n",
        "      x = np.append(x, y)\n",
        "      x = x.reshape(1, -1)\n",
        "      x=scaler.transform(x)\n",
        "      x = x.reshape(-1, 1)\n",
        "      x=x[1:]\n",
        "      x=torch.tensor(x, dtype=torch.float32)\n",
        "      x=x.unsqueeze(0)\n",
        "      x=x.to(device)\n",
        "    y_new_pred=np.append(y_new_pred, y_new_pred[-1])\n",
        "    y_predicted_list.append(y_predicted_list[-1])\n",
        "    draw(scaler.transform(y_new_pred.reshape(1, -1)), scaler.transform(np.array(y_predicted_list).reshape(1, -1))) \n",
        "    break\n",
        "  \n",
        "predict_graph(test_loader, scaler, modelTimeSeriesLSTM)"
      ],
      "metadata": {
        "id": "VomuQAvlNe3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "predict_graph(test_loader, scaler, modelTimeSeriesGRU)"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "V2I2OFSowkcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVluYup-lbU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}